{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haowa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: [WinError 127] Die angegebene Prozedur wurde nicht gefunden\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms.functional as fn\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from utilities import createAnnotation\n",
    "from model_loader import get_new_model\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from PIL import Image \n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "IMAGESROOTDIR = 'NINCO_OOD_classes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, rootDir):\n",
    "        self.rootDir = rootDir\n",
    "        createAnnotation(self.rootDir)\n",
    "        self.annotation =  pd.read_csv('output.csv')\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_path = self.annotation.iloc[index,0]\n",
    "        image = Image.open(data_path)\n",
    "        label = self.annotation.iloc[index,1]\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# instance of class ImageDataset\n",
    "# contains all 765 images with their respective labels\n",
    "imageDataset = ImageDataset(rootDir=IMAGESROOTDIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Constants for the size of the images\n",
    "SIZE = round(224/0.875)\n",
    "\n",
    "# given an Index returns the transformed Image\n",
    "# input: Index: int\n",
    "# return: tuple(PIL Image, label)\n",
    "def transform(index):\n",
    "    assert index <= len(imageDataset)\n",
    "    image, label = imageDataset[index]\n",
    "    rescaledImage = fn.resize(img=image, size=[SIZE, SIZE], interpolation=T.InterpolationMode.BICUBIC)\n",
    "    transformedImage = fn.center_crop(img=rescaledImage, output_size=[SIZE,SIZE])\n",
    "    return transformedImage,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# objects for tensor transformation\n",
    "pilToTensor = T.ToTensor()\n",
    "tensorToPil = T.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Class which is used to get the resized images with label\n",
    "# input: datasetLength: int\n",
    "# output:{'image': Tensor, 'label': String}\n",
    "class DataLoader(Dataset):\n",
    "    def __init__(self, datasetLength):\n",
    "        self.datasetLength = datasetLength\n",
    "   \n",
    "    def __getitem__(self, index):\n",
    "        assert (0 < index <= self.datasetLength)\n",
    "        self.index = index\n",
    "        (picture, label) = transform(index)\n",
    "        image = pilToTensor(picture)\n",
    "        #print(image.shape)\n",
    "        #print(image)\n",
    "        sample3dim = {'image' : image, 'label' : label}\n",
    "        image = image.unsqueeze(0)\n",
    "        #print(image.shape)\n",
    "        #print(image)\n",
    "        sample = {'image': image, 'label': label}\n",
    "        return sample, sample3dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256, 256])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "tensor([[[[0.3529, 0.3686, 0.3922,  ..., 0.6118, 0.5961, 0.5882],\n",
      "          [0.3765, 0.4235, 0.4353,  ..., 0.6314, 0.6235, 0.6157],\n",
      "          [0.3608, 0.4039, 0.3922,  ..., 0.6353, 0.6392, 0.6431],\n",
      "          ...,\n",
      "          [0.8275, 0.8157, 0.8078,  ..., 0.8000, 0.8118, 0.8275],\n",
      "          [0.7843, 0.7765, 0.7686,  ..., 0.7922, 0.8039, 0.8235],\n",
      "          [0.7647, 0.7686, 0.7647,  ..., 0.7882, 0.8000, 0.8196]],\n",
      "\n",
      "         [[0.2431, 0.2588, 0.2824,  ..., 0.4745, 0.4588, 0.4549],\n",
      "          [0.2667, 0.3137, 0.3255,  ..., 0.4941, 0.4902, 0.4784],\n",
      "          [0.2510, 0.2941, 0.2824,  ..., 0.4941, 0.4941, 0.5020],\n",
      "          ...,\n",
      "          [0.8275, 0.8157, 0.8078,  ..., 0.5922, 0.6039, 0.6157],\n",
      "          [0.7843, 0.7765, 0.7686,  ..., 0.5882, 0.5961, 0.6078],\n",
      "          [0.7647, 0.7686, 0.7647,  ..., 0.5843, 0.5922, 0.6039]],\n",
      "\n",
      "         [[0.1961, 0.2118, 0.2353,  ..., 0.3412, 0.3333, 0.3255],\n",
      "          [0.2196, 0.2667, 0.2784,  ..., 0.3608, 0.3529, 0.3490],\n",
      "          [0.2039, 0.2471, 0.2353,  ..., 0.3608, 0.3608, 0.3725],\n",
      "          ...,\n",
      "          [0.8196, 0.8078, 0.8000,  ..., 0.5373, 0.5490, 0.5608],\n",
      "          [0.7843, 0.7725, 0.7608,  ..., 0.5333, 0.5412, 0.5569],\n",
      "          [0.7647, 0.7647, 0.7569,  ..., 0.5294, 0.5373, 0.5490]]]])\n",
      "torch.Size([3, 256, 256])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "tensor([[[[0.8471, 0.8471, 0.8431,  ..., 0.8824, 0.8824, 0.8784],\n",
      "          [0.8431, 0.8431, 0.8471,  ..., 0.8824, 0.8824, 0.8824],\n",
      "          [0.8431, 0.8431, 0.8471,  ..., 0.8824, 0.8824, 0.8824],\n",
      "          ...,\n",
      "          [0.8275, 0.8314, 0.8314,  ..., 0.8902, 0.8902, 0.8902],\n",
      "          [0.8275, 0.8314, 0.8314,  ..., 0.8902, 0.8902, 0.8902],\n",
      "          [0.8275, 0.8314, 0.8314,  ..., 0.8941, 0.8902, 0.8863]],\n",
      "\n",
      "         [[0.8510, 0.8510, 0.8471,  ..., 0.8784, 0.8784, 0.8745],\n",
      "          [0.8471, 0.8471, 0.8510,  ..., 0.8784, 0.8784, 0.8784],\n",
      "          [0.8471, 0.8471, 0.8510,  ..., 0.8784, 0.8784, 0.8784],\n",
      "          ...,\n",
      "          [0.8314, 0.8353, 0.8353,  ..., 0.8863, 0.8863, 0.8863],\n",
      "          [0.8314, 0.8353, 0.8353,  ..., 0.8863, 0.8863, 0.8863],\n",
      "          [0.8314, 0.8353, 0.8353,  ..., 0.8902, 0.8863, 0.8824]],\n",
      "\n",
      "         [[0.8667, 0.8667, 0.8627,  ..., 0.8980, 0.9020, 0.8980],\n",
      "          [0.8627, 0.8627, 0.8667,  ..., 0.8980, 0.9020, 0.9020],\n",
      "          [0.8627, 0.8627, 0.8667,  ..., 0.9020, 0.9020, 0.9020],\n",
      "          ...,\n",
      "          [0.8471, 0.8510, 0.8510,  ..., 0.9098, 0.9098, 0.9098],\n",
      "          [0.8471, 0.8510, 0.8510,  ..., 0.9098, 0.9098, 0.9098],\n",
      "          [0.8471, 0.8510, 0.8510,  ..., 0.9137, 0.9098, 0.9059]]]])\n",
      "torch.Size([3, 256, 256])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "tensor([[[[0.9490, 0.9608, 0.9294,  ..., 0.7333, 0.7333, 0.7294],\n",
      "          [0.9412, 0.9529, 0.9529,  ..., 0.7333, 0.7294, 0.7216],\n",
      "          [0.9725, 0.9725, 0.9725,  ..., 0.7294, 0.7176, 0.7137],\n",
      "          ...,\n",
      "          [0.3020, 0.3059, 0.5333,  ..., 0.8078, 0.8157, 0.8157],\n",
      "          [0.3059, 0.2863, 0.3608,  ..., 0.8118, 0.8235, 0.8196],\n",
      "          [0.3373, 0.2941, 0.2745,  ..., 0.8118, 0.8196, 0.8118]],\n",
      "\n",
      "         [[0.7529, 0.7569, 0.7255,  ..., 0.6353, 0.6353, 0.6314],\n",
      "          [0.7412, 0.7529, 0.7569,  ..., 0.6353, 0.6314, 0.6235],\n",
      "          [0.7725, 0.7725, 0.7765,  ..., 0.6314, 0.6196, 0.6157],\n",
      "          ...,\n",
      "          [0.0706, 0.0902, 0.3412,  ..., 0.7333, 0.7490, 0.7529],\n",
      "          [0.0549, 0.0392, 0.1412,  ..., 0.7451, 0.7569, 0.7608],\n",
      "          [0.0627, 0.0353, 0.0431,  ..., 0.7451, 0.7529, 0.7529]],\n",
      "\n",
      "         [[0.4627, 0.4667, 0.4314,  ..., 0.5176, 0.5176, 0.5137],\n",
      "          [0.4941, 0.4980, 0.4863,  ..., 0.5137, 0.5098, 0.5059],\n",
      "          [0.5647, 0.5490, 0.5412,  ..., 0.5098, 0.5020, 0.4941],\n",
      "          ...,\n",
      "          [0.0588, 0.0627, 0.2745,  ..., 0.6471, 0.6353, 0.6196],\n",
      "          [0.0745, 0.0667, 0.1451,  ..., 0.6471, 0.6392, 0.6118],\n",
      "          [0.1176, 0.0980, 0.0980,  ..., 0.6392, 0.6275, 0.6039]]]])\n",
      "torch.Size([3, 256, 256])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "tensor([[[[0.5333, 0.5333, 0.5451,  ..., 0.6314, 0.6314, 0.6353],\n",
      "          [0.5569, 0.5647, 0.5725,  ..., 0.6314, 0.6353, 0.6353],\n",
      "          [0.5843, 0.6353, 0.5843,  ..., 0.6275, 0.6353, 0.6353],\n",
      "          ...,\n",
      "          [0.1255, 0.1059, 0.1098,  ..., 0.2471, 0.2196, 0.2118],\n",
      "          [0.1529, 0.1647, 0.1647,  ..., 0.2353, 0.2039, 0.1804],\n",
      "          [0.1490, 0.1686, 0.1765,  ..., 0.2118, 0.2000, 0.1843]],\n",
      "\n",
      "         [[0.4000, 0.3961, 0.4078,  ..., 0.7059, 0.7059, 0.7098],\n",
      "          [0.4157, 0.4275, 0.4431,  ..., 0.7059, 0.7098, 0.7098],\n",
      "          [0.4314, 0.4941, 0.4549,  ..., 0.7020, 0.7098, 0.7098],\n",
      "          ...,\n",
      "          [0.1216, 0.1059, 0.1137,  ..., 0.2353, 0.2078, 0.2000],\n",
      "          [0.1608, 0.1725, 0.1725,  ..., 0.2275, 0.1961, 0.1647],\n",
      "          [0.1608, 0.1765, 0.1843,  ..., 0.2039, 0.1922, 0.1686]],\n",
      "\n",
      "         [[0.2588, 0.2510, 0.2471,  ..., 0.8314, 0.8314, 0.8353],\n",
      "          [0.2706, 0.2784, 0.2824,  ..., 0.8314, 0.8353, 0.8353],\n",
      "          [0.2667, 0.3255, 0.2902,  ..., 0.8196, 0.8275, 0.8275],\n",
      "          ...,\n",
      "          [0.0275, 0.0078, 0.0157,  ..., 0.1765, 0.1529, 0.1451],\n",
      "          [0.0549, 0.0745, 0.0824,  ..., 0.1725, 0.1373, 0.1137],\n",
      "          [0.0549, 0.0784, 0.0902,  ..., 0.1490, 0.1333, 0.1176]]]])\n"
     ]
    }
   ],
   "source": [
    "# Amount of random samples \n",
    "BATCHSIZE = 4\n",
    "\n",
    "dataloader = DataLoader(len(imageDataset))\n",
    "\n",
    "'''\n",
    "function creates a random batch of data with a given size\n",
    "Arguments: batchsize:int\n",
    "Return: an array with a dict[image:label] \n",
    "'''\n",
    "def createRandomBatch(batchsize):\n",
    "    assert (0<batchsize <= len(imageDataset))\n",
    "    batch = []\n",
    "    batch3dim = []\n",
    "    indexList = []\n",
    "    for i in range(batchsize):\n",
    "        index = random.randint(0,len(imageDataset))\n",
    "        indexList.append(index)\n",
    "        sample, sample3dim = dataloader[index]\n",
    "        batch.append(sample)\n",
    "        batch3dim.append(sample3dim)\n",
    "    return batch, batch3dim, indexList\n",
    "\n",
    "samples, samples3dim, indexList = createRandomBatch(BATCHSIZE)\n",
    "  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# loads pretrained model\n",
    "model = get_new_model(\"convnext_tiny\", not_original=True)\n",
    "\n",
    "\n",
    "'''\n",
    "function feeds the loaded model with data\n",
    "Arguments: list[dict[image:tensor,label:str]]\n",
    "Return: None\n",
    "'''\n",
    "def feedModel(samples):\n",
    "    assert(0<len(samples)<len(imageDataset))\n",
    "    for sample in samples:\n",
    "        image, label = sample['image'], sample['label']\n",
    "        prediction = model(image)\n",
    "        sample[\"prediction\"]=prediction\n",
    "        #print(sample[\"prediction\"].shape)\n",
    "    return samples\n",
    "        \n",
    "        \n",
    "        \n",
    "newSamples = feedModel(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (256) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 26\u001b[0m\n\u001b[0;32m     22\u001b[0m     plt\u001b[39m.\u001b[39mimshow(grid\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mtranspose((\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m0\u001b[39m)))\n\u001b[0;32m     25\u001b[0m plt\u001b[39m.\u001b[39mfigure()\n\u001b[1;32m---> 26\u001b[0m visualize(samples)\n\u001b[0;32m     27\u001b[0m extractValuesFromDict(samples3dim, \u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     28\u001b[0m plt\u001b[39m.\u001b[39maxis(\u001b[39m'\u001b[39m\u001b[39moff\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[41], line 21\u001b[0m, in \u001b[0;36mvisualize\u001b[1;34m(samples)\u001b[0m\n\u001b[0;32m     19\u001b[0m grid_border_size \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m     20\u001b[0m elementsPerRow \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n\u001b[1;32m---> 21\u001b[0m grid \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mmake_grid(tensor\u001b[39m=\u001b[39;49mtensors, nrow\u001b[39m=\u001b[39;49melementsPerRow, padding\u001b[39m=\u001b[39;49mgrid_border_size)\n\u001b[0;32m     22\u001b[0m plt\u001b[39m.\u001b[39mimshow(grid\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mtranspose((\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m0\u001b[39m)))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\utils.py:130\u001b[0m, in \u001b[0;36mmake_grid\u001b[1;34m(tensor, nrow, padding, normalize, value_range, scale_each, pad_value, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    126\u001b[0m         \u001b[39m# Tensor.copy_() is a valid method but seems to be missing from the stubs\u001b[39;00m\n\u001b[0;32m    127\u001b[0m         \u001b[39m# https://pytorch.org/docs/stable/tensors.html#torch.Tensor.copy_\u001b[39;00m\n\u001b[0;32m    128\u001b[0m         grid\u001b[39m.\u001b[39;49mnarrow(\u001b[39m1\u001b[39;49m, y \u001b[39m*\u001b[39;49m height \u001b[39m+\u001b[39;49m padding, height \u001b[39m-\u001b[39;49m padding)\u001b[39m.\u001b[39;49mnarrow(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m    129\u001b[0m             \u001b[39m2\u001b[39;49m, x \u001b[39m*\u001b[39;49m width \u001b[39m+\u001b[39;49m padding, width \u001b[39m-\u001b[39;49m padding\n\u001b[1;32m--> 130\u001b[0m         )\u001b[39m.\u001b[39;49mcopy_(tensor[k])\n\u001b[0;32m    131\u001b[0m         k \u001b[39m=\u001b[39m k \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    132\u001b[0m \u001b[39mreturn\u001b[39;00m grid\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (256) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "'''\n",
    "function extracts the values from the samples dict\n",
    "Arguments: dict which contains random batch dict\n",
    "Return: returns the values from samples list\n",
    "'''\n",
    "def extractValuesFromDict(samples, key:str):\n",
    "    values = []\n",
    "    for dictionary in samples:\n",
    "        values.append(dictionary[key])\n",
    "        \n",
    "    if key == 'label':\n",
    "        print(values)\n",
    "    return values\n",
    "\n",
    "\n",
    "# function to visualize the batch\n",
    "def visualize(samples):\n",
    "    tensors = extractValuesFromDict(samples, 'image')\n",
    "    grid_border_size = 2\n",
    "    elementsPerRow = 4\n",
    "    grid = torchvision.utils.make_grid(tensor=tensors, nrow=elementsPerRow, padding=grid_border_size)\n",
    "    plt.imshow(grid.detach().numpy().transpose((1,2,0)))\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "visualize(samples3dim)\n",
    "extractValuesFromDict(samples3dim, 'label')\n",
    "plt.axis('off')\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
