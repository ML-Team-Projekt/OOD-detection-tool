{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms.functional as fn\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from utilities import createAnnotation\n",
    "from model_loader import get_new_model\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from PIL import Image \n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "IMAGESROOTDIR = 'NINCO_OOD_classes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, rootDir):\n",
    "        self.rootDir = rootDir\n",
    "        createAnnotation(self.rootDir)\n",
    "        self.annotation =  pd.read_csv('output.csv')\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_path = self.annotation.iloc[index,0]\n",
    "        image = Image.open(data_path)\n",
    "        label = self.annotation.iloc[index,1]\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# instance of class ImageDataset\n",
    "# contains all 765 images with their respective labels\n",
    "imageDataset = ImageDataset(rootDir=IMAGESROOTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Class which is used to rescale the image to a given size\n",
    "# input: outputSize: int\n",
    "# return: tuple(PIL Image, label)\n",
    "class Rescale:\n",
    "    def __init__(self, outputSize):\n",
    "        self.outputSize = outputSize   \n",
    "        \n",
    "        \n",
    "    def __calculateNewSize(self, size):\n",
    "        initialWidth, initialHeight = size\n",
    "        \n",
    "        \n",
    "        RATIO = initialWidth/self.outputSize\n",
    "        newWidth = self.outputSize\n",
    "        newHeight = initialHeight/RATIO\n",
    "        \n",
    "        return (round(newWidth), round(newHeight))  \n",
    "        \n",
    "    #sample data is a tuple(image, label)\n",
    "    def __call__(self, sampleData):\n",
    "        image, label = sampleData\n",
    "        \n",
    "        size = image.size\n",
    "        \n",
    "        newWidth, newHeight = self.__calculateNewSize(size)\n",
    "        \n",
    "        transformedImage = fn.resize(image, [newHeight, newWidth])\n",
    "        \n",
    "        return transformedImage,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Class which is used to center crop non quadratic images\n",
    "# input: outputSize: int\n",
    "# return: tuple(PIL Image, label)\n",
    "class CenterCrop:\n",
    "    def __init__(self, outputSize):\n",
    "        self.outputSize = outputSize\n",
    "        \n",
    "    # creates a quadratic image\n",
    "    def __call__(self, sampleData):\n",
    "        image, label = sampleData\n",
    "        \n",
    "        width, height = image.size\n",
    "        \n",
    "        if (width != height or width != self.outputSize):\n",
    "            centerCrop = torchvision.transforms.CenterCrop(self.outputSize)\n",
    "\n",
    "            return centerCrop(image), label\n",
    "        return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Constants for the size of the images\n",
    "RESCALE = 240\n",
    "CROP = 240\n",
    "# objects for resizing\n",
    "rescale = Rescale(RESCALE)\n",
    "crop = CenterCrop(CROP)\n",
    "composed = T.Compose([rescale, crop])\n",
    "\n",
    "# given an Index returns the transformed Image\n",
    "# input: Index: int\n",
    "# return: tuple(PIL Image, label)\n",
    "def transform(index):\n",
    "    assert index <= len(imageDataset)\n",
    "    tmp = composed(imageDataset[index])\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# objects for tensor transformation\n",
    "pilToTensor = T.ToTensor()\n",
    "tensorToPil = T.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Class which is used to get the resized images with label\n",
    "# input: datasetLength: int\n",
    "# output:{'image': Tensor, 'label': String}\n",
    "class DataLoader(Dataset):\n",
    "    def __init__(self, datasetLength):\n",
    "        self.datasetLength = datasetLength\n",
    "   \n",
    "    def __getitem__(self, index):\n",
    "        assert (0 < index <= self.datasetLength)\n",
    "        self.index = index\n",
    "        (picture, label) = transform(index)\n",
    "        image = pilToTensor(picture)\n",
    "        sample3dim = {'image' : image, 'label' : label}\n",
    "        image = image.unsqueeze(0)\n",
    "        sample = {'image': image, 'label': label}\n",
    "        return sample, sample3dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Amount of random samples \n",
    "BATCHSIZE = 4\n",
    "\n",
    "dataloader = DataLoader(len(imageDataset))\n",
    "\n",
    "'''\n",
    "function creates a random batch of data with a given size\n",
    "Arguments: batchsize:int\n",
    "Return: an array with a dict[image:label] \n",
    "'''\n",
    "def createRandomBatch(batchsize):\n",
    "    assert (0<batchsize <= len(imageDataset))\n",
    "    batch = []\n",
    "    batch3dim = []\n",
    "    for i in range(batchsize):\n",
    "        index = random.randint(0,len(imageDataset))\n",
    "        sample, sample3dim = dataloader[index]\n",
    "        batch.append(sample)\n",
    "        batch3dim.append(sample3dim)\n",
    "    return batch, batch3dim\n",
    "\n",
    "samples, samples3dim = createRandomBatch(BATCHSIZE)\n",
    "  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# loads pretrained model\n",
    "model = get_new_model(\"convnext_tiny\", not_original=True)\n",
    "\n",
    "\n",
    "'''\n",
    "function feeds the loaded model with data\n",
    "Arguments: list[dict[image:tensor,label:str]]\n",
    "Return: None\n",
    "'''\n",
    "def feedModel(samples):\n",
    "    assert(0<len(samples)<len(imageDataset))\n",
    "    for sample in samples:\n",
    "        image, label = sample['image'], sample['label']\n",
    "        model(image)\n",
    "        \n",
    "feedModel(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([3, 240, 240]) lepomis_auritus\n",
      "1 torch.Size([3, 240, 240]) Caracal caracal caracal\n",
      "2 torch.Size([3, 240, 240]) darlingtonia_californica\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(len(imageDataset))\n",
    "\n",
    "# function to visualize the batch\n",
    "def visualize(samples):\n",
    "    im_size = samples[0]['image'].size(2)\n",
    "    grid_border_size = 2\n",
    "    grid = torchvision.utils.make_grid(samples[0]['image'])\n",
    "    plt.imshow(grid.detach().numpy().transpose((1,2,0)))\n",
    "    \n",
    "    for i in range(BATCHSIZE-1):\n",
    "        plt.scatter(samples[i]['image'][i, :, 0].detach().numpy() + (i) * im_size + \n",
    "                    (i + 1) * grid_border_size,\n",
    "                    samples[i]['image'][i, :, 1].detach().numpy() + \n",
    "                    grid_border_size, s=10, marker = '.', c='r')\n",
    "        plt.title('Batch')\n",
    "        \n",
    "for i in range(BATCHSIZE - 1):\n",
    "    print(i, samples3dim[i]['image'].size(), samples3dim[i]['label'])\n",
    "    \n",
    "    if i == BATCHSIZE:\n",
    "        plt.figure()\n",
    "        visualize(samples3dim)\n",
    "        plt.axis('off')\n",
    "        plt.ioff()\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
