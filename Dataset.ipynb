{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms.functional as fn\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from utilities import createAnnotation\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from PIL import Image \n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGESROOTDIR = 'NINCO_OOD_classes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, rootDir):\n",
    "        self.rootDir = rootDir\n",
    "        createAnnotation(self.rootDir)\n",
    "        self.annotation =  pd.read_csv('output.csv')\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_path = self.annotation.iloc[index,0]\n",
    "        image = Image.open(data_path)\n",
    "        label = self.annotation.iloc[index,1]\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instance of class ImageDataset\n",
    "# contains all 765 images with their respective labels\n",
    "imageDataset = ImageDataset(rootDir=IMAGESROOTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class which is used to rescale the image to a given size\n",
    "# input: outputSize: int\n",
    "# return: tuple(PIL Image, label)\n",
    "class Rescale:\n",
    "    def __init__(self, outputSize):\n",
    "        self.outputSize = outputSize   \n",
    "        \n",
    "        \n",
    "    def __calculateNewSize(self, size):\n",
    "        initialWidth, initialHeight = size\n",
    "        \n",
    "        \n",
    "        RATIO = initialWidth/self.outputSize\n",
    "        newWidth = self.outputSize\n",
    "        newHeight = initialHeight/RATIO\n",
    "        \n",
    "        return (round(newWidth), round(newHeight))  \n",
    "        \n",
    "    #sample data is a tuple(image, label)\n",
    "    def __call__(self, sampleData):\n",
    "        image, label = sampleData\n",
    "        \n",
    "        size = image.size\n",
    "        \n",
    "        newWidth, newHeight = self.__calculateNewSize(size)\n",
    "        \n",
    "        transformedImage = fn.resize(image, [newHeight, newWidth])\n",
    "        \n",
    "        return transformedImage,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class which is used to center crop non quadratic images\n",
    "# input: outputSize: int\n",
    "# return: tuple(PIL Image, label)\n",
    "class CenterCrop:\n",
    "    def __init__(self, outputSize):\n",
    "        self.outputSize = outputSize\n",
    "        \n",
    "    # creates a quadratic image\n",
    "    def __call__(self, sampleData):\n",
    "        image, label = sampleData\n",
    "        \n",
    "        width, height = image.size\n",
    "        \n",
    "        if (width != height or width != self.outputSize):\n",
    "            centerCrop = torchvision.transforms.CenterCrop(self.outputSize)\n",
    "\n",
    "            return centerCrop(image), label\n",
    "        return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for the size of the images\n",
    "RESCALE = 240\n",
    "CROP = 240\n",
    "# objects for resizing\n",
    "rescale = Rescale(RESCALE)\n",
    "crop = CenterCrop(CROP)\n",
    "composed = T.Compose([rescale, crop])\n",
    "\n",
    "# given an Index returns the transformed Immage\n",
    "# input: Index: int\n",
    "# return: tuple(PIL Image, label)\n",
    "def transform(index):\n",
    "    assert index <= len(imageDataset)\n",
    "    tmp = composed(imageDataset[index])\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objects for tensor transformation\n",
    "pilToTensor = T.ToTensor()\n",
    "tensorToPil = T.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class which is used to get the resized images with label\n",
    "# input: datasetLength: int\n",
    "# output:{'image': Tensor, 'label': String}\n",
    "class DataLoader(Dataset):\n",
    "    def __init__(self, datasetLength):\n",
    "        self.datasetLength = datasetLength\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        assert (0 < index <= self.datasetLength)\n",
    "        self.index = index\n",
    "        (picture, label) = transform(index)\n",
    "        image = pilToTensor(picture)\n",
    "        sample = {'image': image, 'label': label}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount of random samples \n",
    "BATCHSIZE = 4\n",
    "\n",
    "dataloader = DataLoader(len(imageDataset))\n",
    "for i in range(BATCHSIZE):\n",
    "    index = random.randint(0,len(imageDataset))\n",
    "    sample = dataloader[index]\n",
    "  \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
